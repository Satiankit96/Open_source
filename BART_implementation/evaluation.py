from rouge_score import rouge_scorer
from bert_score import score as bert_score
import sacrebleu

def calculate_rouge(generated_summary, ground_truth):
    """
    Calculates ROUGE scores between the generated summary and the ground truth.

    Args:
        generated_summary (str): The summary generated by the model.
        ground_truth (str): The human-generated summary to compare against.

    Returns:
        float: The ROUGE-1 F1 score for n-gram overlap.
    """
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(generated_summary, ground_truth)
    return scores['rouge1'].fmeasure

def calculate_bert_score(generated_summary, ground_truth):
    """
    Calculates the BERTScore between the generated summary and the ground truth.

    Args:
        generated_summary (str): The summary generated by the model.
        ground_truth (str): The human-generated summary to compare against.

    Returns:
        float: The F1 score from BERTScore.
    """
    P, R, F1 = bert_score([generated_summary], [ground_truth], lang="en", model_type="bert-base-uncased")
    return F1.mean().item()

def calculate_bleu(generated_summary, ground_truth):
    """
    Calculates the BLEU score between the generated summary and the ground truth.

    Args:
        generated_summary (str): The summary generated by the model.
        ground_truth (str): The human-generated summary to compare against.

    Returns:
        float: The BLEU score.
    """
    bleu = sacrebleu.corpus_bleu([generated_summary], [[ground_truth]])
    return bleu.score
